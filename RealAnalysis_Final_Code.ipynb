{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch versionï¼š\n",
      "2.5.0+cu124\n",
      "CUDA Version: \n",
      "12.4\n",
      "cuDNN version is :\n",
      "90100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Pytorch versionï¼š\")\n",
    "print(torch.__version__)\n",
    "print(\"CUDA Version: \")\n",
    "print(torch.version.cuda)\n",
    "print(\"cuDNN version is :\")\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/cs505aw/students/ziyechen/.conda/envs/myenv310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 10240 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.10.0: Fast Qwen2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA L40S. Max memory: 44.418 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.10.0 patched 28 layers with 0 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"MathAnalysis_Qwen_Classifier\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 10240,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 1006.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "evaluation_prompt = \"\"\"As a mathematical assistant, You need to analyze the problem to find out what type of problem it belongs to in Real Analysis. Provide the Problem_Type and the Knowledges which may be used to solve this problem.\n",
    "\n",
    "### Problem:\n",
    "{}\n",
    "\n",
    "### Problem_Type:\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    Problem       = examples[\"Problem\"]\n",
    "    texts = []\n",
    "    for problem in Problem:\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = evaluation_prompt.format(problem) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv',data_files = './benchmark_data.csv', split='train')\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number': ['1_4', '1_6'],\n",
       " 'ProblemType': ['Sequences and Limits', 'Sequences and Limits'],\n",
       " 'Problem': ['Suppose that $a_n$ and $b_n$ converge to $\\\\alpha$ and $\\\\beta$ as $n \\\\to \\\\infty$ respectively. Show that the sequence\\n$$\\n\\\\frac{a_0 b_n + a_1 b_{n-1} + \\\\cdots + a_n b_0}{n}\\n$$\\nconverges to $\\\\alpha \\\\beta$ as $n \\\\to \\\\infty$.',\n",
       "  \"For any positive sequence $\\\\{a_n\\\\}_{n \\\\geq 1}$, show that\\n$$\\n\\\\left( \\\\frac{a_1 + a_{n+1}}{a_n} \\\\right)^n > e\\n$$\\nfor infinitely many $n$'s, where $e$ is the base of the natural logarithm. Prove moreover that the constant $e$ on the right-hand side cannot in general be replaced by any larger number.\"],\n",
       " 'Solution': ['Let $M$ be an upper bound of the two convergent sequences $|a_n|$ and $|b_n|$. For any $\\\\epsilon > 0$ we can take a positive integer $N$ satisfying $|a_n - \\\\alpha| < \\\\epsilon$ and $|b_n - \\\\beta| < \\\\epsilon$ for all integers $n$ greater than $N$. If $n$ is greater than $N^2$, then\\n$$\\n|a_k b_{n-k} - \\\\alpha \\\\beta| \\\\leq |(a_k - \\\\alpha) b_{n-k} + \\\\alpha (b_{n-k} - \\\\beta)| \\n\\\\leq (M + |\\\\alpha|) \\\\epsilon\\n$$\\nfor any integer $k$ in the interval $\\\\left[\\\\sqrt{n}, n - \\\\sqrt{n}\\\\right]$. Therefore\\n$$\\n\\\\left| \\\\frac{1}{n} \\\\sum_{k=0}^n a_k b_{n-k} - \\\\alpha \\\\beta \\\\right| \\n\\\\leq \\\\frac{1}{n} \\\\sum_{\\\\sqrt{n} \\\\leq k \\\\leq n - \\\\sqrt{n}} |a_k b_{n-k} - \\\\alpha \\\\beta|\\n+ 2 \\\\left(|\\\\alpha \\\\beta| + M^2 \\\\right) \\\\frac{\\\\lfloor \\\\sqrt{n} \\\\rfloor + 1}{n}\\n$$\\n$$\\n\\\\leq (M + |\\\\alpha|) \\\\epsilon + 2 \\\\left(|\\\\alpha \\\\beta| + M^2\\\\right) \\\\frac{\\\\sqrt{n} + 1}{n}.\\n$$\\n\\nWe can take $n$ so large that the last expression is less than $(M + |\\\\alpha| + 1)\\\\epsilon$.',\n",
       "  'Without loss of generality we may put $a_1 = 1$. Suppose, contrary to the conclusion, that there is an integer $N$ satisfying\\n$$\\n\\\\left(\\\\frac{1 + a_{n+1}}{a_n}\\\\right)^n \\\\leq e\\n$$\\nfor all $n \\\\geq N$. Put\\n$$\\ns_{j,k} = \\\\exp\\\\left(\\\\frac{1}{j} + \\\\cdots + \\\\frac{1}{k}\\\\right)\\n$$\\nfor any integers $j \\\\leq k$. Since $0 < a_{n+1} \\\\leq e^{1/n} a_n - 1$, we get successively\\n$$\\n\\\\begin{cases}\\n0 < a_{n+1} \\\\leq s_{n,n} a_n - 1, \\\\\\\\\\n0 < a_{n+2} \\\\leq s_{n,n+1} a_n - s_{n+1,n+1} - 1, \\\\\\\\\\n\\\\vdots \\\\\\\\\\n0 < a_{n+k+1} \\\\leq s_{n,n+k} a_n - s_{n+1,n+k} - \\\\cdots - s_{n+k,n+k} - 1\\n\\\\end{cases}\\n$$\\nfor any non-negative integer $k$. Hence it follows that\\n$$\\na_n > \\\\frac{1}{s_{n,n}} + \\\\frac{1}{s_{n,n+1}} + \\\\cdots + \\\\frac{1}{s_{n,n+k}}.\\n$$\\n\\nOn the other hand, using the inequality\\n$$\\n\\\\frac{1}{s_{n,n+j}} > \\\\exp\\\\left(-\\\\int_{n-1}^{n+j} \\\\frac{dx}{x}\\\\right) = \\\\frac{n-1}{n+j},\\n$$\\nwe get\\n$$\\na_n > \\\\sum_{j=0}^k \\\\frac{n-1}{n+j},\\n$$\\nwhich is a contradiction, since the right-hand side diverges to $\\\\infty$ as $k \\\\to \\\\infty$.\\n\\nTo see that the bound $e$ cannot be replaced by any larger number, consider the case $a_n = n \\\\log n$ for $n \\\\geq 2$. Then\\n$$\\n\\\\left(\\\\frac{a_1 + (n+1)\\\\log(n+1)}{n \\\\log n}\\\\right)^n = \\\\exp\\\\left(n \\\\log\\\\left(1 + \\\\frac{1}{n} + O\\\\left(\\\\frac{1}{n \\\\log n}\\\\right)\\\\right)\\\\right)\\n$$\\n$$\\n= \\\\exp\\\\left(1 + O\\\\left(\\\\frac{1}{\\\\log n}\\\\right)\\\\right),\\n$$\\nwhich converges to $e$ as $n \\\\to \\\\infty$.\\n'],\n",
       " 'text': ['As a mathematical assistant, You need to analyze the problem to find out what type of problem it belongs to in Real Analysis. Provide the Problem_Type and the Knowledges which may be used to solve this problem.\\n\\n### Problem:\\nSuppose that $a_n$ and $b_n$ converge to $\\\\alpha$ and $\\\\beta$ as $n \\\\to \\\\infty$ respectively. Show that the sequence\\n$$\\n\\\\frac{a_0 b_n + a_1 b_{n-1} + \\\\cdots + a_n b_0}{n}\\n$$\\nconverges to $\\\\alpha \\\\beta$ as $n \\\\to \\\\infty$.\\n<|endoftext|>',\n",
       "  \"As a mathematical assistant, You need to analyze the problem to find out what type of problem it belongs to in Real Analysis. Provide the Problem_Type and the Knowledges which may be used to solve this problem.\\n\\n### Problem:\\nFor any positive sequence $\\\\{a_n\\\\}_{n \\\\geq 1}$, show that\\n$$\\n\\\\left( \\\\frac{a_1 + a_{n+1}}{a_n} \\\\right)^n > e\\n$$\\nfor infinitely many $n$'s, where $e$ is the base of the natural logarithm. Prove moreover that the constant $e$ on the right-hand side cannot in general be replaced by any larger number.\\n<|endoftext|>\"]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "TYPE = []\n",
    "KNOWLEDGE = []\n",
    "for row in dataset:\n",
    "    number = row['Number']\n",
    "    problem = row['Problem']\n",
    "    \n",
    "    # Format the prompt\n",
    "    prompt = evaluation_prompt.format(problem)\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    # Generate the solution\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        # streamer = text_streamer,\n",
    "        max_new_tokens=4096,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        # do_sample=True,   # Enable sampling for more diverse outputs\n",
    "        # temperature=0.7   # Adjust temperature for creativity\n",
    "    )\n",
    "    \n",
    "    # Decode the output\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    # Extract the generated solution\n",
    "    problem_type = output_text[output_text.find('### Problem_Type')+17:output_text.find('### Knowledge')].strip()\n",
    "    knowledge = output_text[output_text.find('### Knowledge')+14:].strip()\n",
    "    \n",
    "    # Append the result\n",
    "    results.append({\n",
    "        'Number': number,\n",
    "        'Problem': problem,\n",
    "        'Problem_Type': problem_type,\n",
    "        'Knowledge': knowledge\n",
    "    })\n",
    "\n",
    "    TYPE.append(problem_type)\n",
    "    KNOWLEDGE.append(knowledge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset)==len(TYPE))\n",
    "print(len(dataset)==len(KNOWLEDGE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.10.0: Fast Qwen2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA L40S. Max memory: 44.418 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: unsloth/Qwen2.5-Math-7B-bnb-4bit can only handle sequence lengths of at most 4096.\n",
      "But with kaiokendev's RoPE scaling of 2.5, it can be magically be extended to 10240!\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"MathAnalysis_Qwen_ProblemSolver\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 10240,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"As a mathematical assistant, solve the following problem. Provide a detailed, step-by-step solution using rigorous mathematical reasoning. If the problem requires the use of the $\\epsilon$-$\\delta$ method (e.g., when proving limits or continuity), ensure that you apply it appropriately. Use precise mathematical language and notation throughout your solution.\n",
    "\n",
    "### Problem_Type:\n",
    "{}\n",
    "\n",
    "### Problem:\n",
    "{}\n",
    "\n",
    "### Knowledge:\n",
    "{}\n",
    "\n",
    "### Solution:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    Problem_Type    = examples[\"ProblemType\"]\n",
    "    Problem       = examples[\"Problem\"]\n",
    "    Knowledge = examples[\"Knowledge\"]\n",
    "    Solution  = examples[\"Solution\"]\n",
    "    texts = []\n",
    "    for problem_type, problem, knowledge, solution in zip(Problem_Type, Problem, Knowledge, Solution):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(problem_type, problem, knowledge, solution) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv',data_files = './benchmark_data_with_knowledge.csv', split='train')\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"As a mathematical assistant, solve the following problem. Provide a detailed, step-by-step solution using rigorous mathematical reasoning. If the problem requires the use of the $\\epsilon$-$\\delta$ method (e.g., when proving limits or continuity), ensure that you apply it appropriately. Use precise mathematical language and notation throughout your solution.\n",
    "\n",
    "### Problem_Type:\n",
    "{}\n",
    "\n",
    "### Problem:\n",
    "{}\n",
    "\n",
    "### Knowledge:\n",
    "{}\n",
    "\n",
    "### Solution:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv',data_files = './benchmark_data.csv', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "N_ROW = 0\n",
    "for row in dataset:\n",
    "    number = row['Number']\n",
    "    problem_type = TYPE[N_ROW]\n",
    "    problem = row['Problem']\n",
    "    knowledge = KNOWLEDGE[N_ROW]\n",
    "    \n",
    "    # Format the prompt\n",
    "    prompt = alpaca_prompt.format(problem_type, problem, knowledge, \"\")\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    # Generate the solution\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        # streamer = text_streamer,\n",
    "        max_new_tokens=2048,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        # do_sample=True,   # Enable sampling for more diverse outputs\n",
    "        # temperature=0.7   # Adjust temperature for creativity\n",
    "    )\n",
    "    \n",
    "    # Decode the output\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    # Extract the generated solution\n",
    "    generated_solution = output_text[len(prompt):].strip()\n",
    "    \n",
    "    # Append the result\n",
    "    results.append({\n",
    "        'Number': number,\n",
    "        'Problem': problem,\n",
    "        'Solution': generated_solution\n",
    "    })\n",
    "\n",
    "    N_ROW += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number': '1_6',\n",
       " 'Problem': \"For any positive sequence $\\\\{a_n\\\\}_{n \\\\geq 1}$, show that\\n$$\\n\\\\left( \\\\frac{a_1 + a_{n+1}}{a_n} \\\\right)^n > e\\n$$\\nfor infinitely many $n$'s, where $e$ is the base of the natural logarithm. Prove moreover that the constant $e$ on the right-hand side cannot in general be replaced by any larger number.\",\n",
       " 'Solution': \"Let $M$ be any arbitrary positive number. It suffices to show that the inequality\\n\\\\[\\n\\\\frac{1}{n} \\\\log \\\\frac{a_1 + a_{n+1}}{a_n} \\\\geq \\\\frac{M}{n}\\n\\\\tag{3.1}\\n\\\\]\\nholds for all sufficiently large $n$. We will prove (3.1) by Cauchy's method. Let $p$ and $q$ be any positive integers; then we have\\n\\\\[\\n\\\\frac{1}{q-p} \\\\sum_{n=p}^{q-1} \\\\frac{1}{n} \\\\log \\\\frac{a_1 + a_{n+1}}{a_n} \\\\geq \\\\frac{1}{p}.\\n\\\\]\\n\\\\]\\nNow let $p$ be a fixed positive integer and $q$ vary. Since the right-hand side converges to $0$ as $q \\\\to \\\\infty$, it follows that the superior limit of of the left-hand side as $q \\\\to \\\\infty$ is $\\\\geq 0$. Therefore there exists an integer $N > p$ satisfying (3.1) for all $q > N$. This is equivalent to saying that the sequence\\n\\\\[\\ns_n = \\\\left( \\\\frac{a_1 + a_{n+1}}{a_n} \\\\right)^n\\n\\\\]\\nis monotone increasing for all $n > N$. Since $M$ is arbitrary, we have\\n\\\\[\\n\\\\liminf_{n \\\\to \\\\inftyty} s_n \\\\leqq \\\\liminf_{n \\\\to \\\\infty} \\\\exp\\\\left(n \\\\log \\\\frac{a_1 + a_{n+1}}{a_n}\\\\right) \\\\leq \\\\exp\\\\left( \\\\liminf_{n \\\\to \\\\infty} \\\\frac{1}{n} \\\\log \\\\frac{a_1 + a_{n+1}}{a_n}} \\\\right) \\\\leq e^M.\\n\\\\]\\nfor any positive $M$.  Hence the sequence $\\\\{s_n\\\\}$ converges and\\n\\\\[\\n\\\\lim_{n \\\\to \\\\infty} s_n = \\\\exp\\\\left( \\\\lim_{n \\\\to \\\\infty} \\\\frac{1}{n} \\\\log \\\\frac{a_1 + a_{n+1}}{a_n} \\\\right) \\\\leq e^M\\n\\\\]\\nfor any positive $M$. This implies that\\n\\\\[\\n\\\\exp\\\\left( \\\\limsup_{n \\\\to \\\\infty} \\\\frac{1}{n} \\\\log \\\\frac{a_1 + a_{n+1}}{a_n} \\\\right) \\\\leq e^M = \\\\exp\\\\left( \\\\limsup_{n \\\\to \\\\infty} \\\\frac{1}{n} \\\\log e^M \\\\right),\\n\\\\]\\nand hence\\n\\\\[\\n\\\\limsup_{n \\\\to \\\\infty} \\\\frac{1}{n} \\\\log \\\\frac{a_1 + a_{n+1}}{a_n} \\\\leq 1.\\n\\\\]\\n\\\\]\\nWe thus have\\n\\\\[\\n\\\\limsup_{n \\\\to \\\\infty} \\\\left( \\\\frac{1}{n} \\\\log \\\\frac{a_1 + a_{n+1}}{a_n} \\\\right)_+1} = \\\\limsup_{n \\\\to \\\\infty \\\\left( \\\\frac{n+1}{n} \\\\right) \\\\left( \\\\frac{1}{n+1} \\\\log \\\\frac{a_1 + a_{n+1}}{a_n} \\\\right)\\n\\\\[\\n= \\\\limsup_{n \\\\to \\\\infty} \\\\frac{1}{n} \\\\log \\\\frac{a_1 + a_{n+1}}{a_n},\\n\\\\]\\nwhich implies that the sequence\\n\\\\[\\ns_n' = \\\\left( \\\\frac{a_1 + a_{n+1}}{a_n} \\\\right)^n\\n\\\\]\\nis also monotoneone increasing for all sufficiently large $n$. Therefore we have $s_n'' > s_n$ for any $p > q$ and all sufficiently large $n$, which implies that\\n\\\\[\\n\\\\left( \\\\frac{a_1 + a_{p+1}}{a_p} \\\\right)^p > \\\\left( \\\\frac{a_1 + a_{q+1}}{a_q} \\\\right)^q\\n\\\\]\\nfor any $q > p$. This means that\\n\\\\[\\n\\\\left( 1 + \\\\frac{a_{p+1}}{a_p} \\\\right)^p > \\\\left( 1 + \\\\frac{a_{q+1}}{a_q} \\\\right)^{q-1},\\n\\\\]\\nand hence\\n\\\\[\\ne > \\\\lim_{n \\\\exp\\\\left( \\\\frac{a_1 + a_{n+1}}{a_n} \\\\right) \\\\geqq \\\\liminf_{n \\\\exp\\\\left( 1 + \\\\frac{a_{n+1}}{a_n} \\\\right)^n}\\n= \\\\exp\\\\left( \\\\liminf_{n \\\\to \\\\infty} \\\\left( 1 + \\\\frac{a_{n+1}}{a_n_n \\\\right)^{1/n} \\\\right),\\n\\\\]\\nsince the sequence $\\\\{(1 + 1/n)^n\\\\}$ converges to $e$ as $n \\\\to \\\\infty$. This completes the proof.\"}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions have been generated and saved to 'solutions.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Save the Results to a New CSV File\n",
    "with open('solutions.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Number','Problem', 'Solution']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for result in results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(\"Solutions have been generated and saved to 'solutions.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
